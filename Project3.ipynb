{"cells":[{"cell_type":"markdown","metadata":{"id":"xNxsoHi21wtR"},"source":["# Import"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20411,"status":"ok","timestamp":1683412598514,"user":{"displayName":"Luca B","userId":"02373234078534734966"},"user_tz":-120},"id":"sQdxtSi81rVj","outputId":"6f6767ca-bd7e-4e3f-8f64-5162d0998acb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["# This serves as a template which will guide you through the implementation of this task.  It is advised\n","# to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps\n","# First, we import necessary libraries:\n","import numpy as np\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, TensorDataset\n","import os\n","import torch\n","from torchvision import transforms\n","import torchvision.datasets as datasets\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import os\n","from torchvision.models import resnet34, ResNet34_Weights\n","from sklearn.preprocessing import normalize\n","from google.colab import drive\n","drive.mount(\"/content/drive/\")\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","sample_dir = \"/content/drive/MyDrive/IML/Projekt 3/dataset/Sample\"\n","dataset_dir = \"/content/drive/MyDrive/IML/Projekt 3/dataset/\"\n","embeddings_dir = \"/content/drive/MyDrive/IML/Projekt 3/dataset/embeddings.npy\"\n","results_dir = \"/content/drive/MyDrive/IML/Projekt 3/dataset/results.txt\"\n","train_dir = \"/content/drive/MyDrive/IML/Projekt 3/train_triplets.txt\" \n","test_dir = \"/content/drive/MyDrive/IML/Projekt 3/test_triplets.txt\""]},{"cell_type":"markdown","metadata":{"id":"CPX_HPVgjdn3"},"source":["# Generate Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJ0f5Utr6qCe"},"outputs":[],"source":["weights = ResNet34_Weights.DEFAULT\n","model = resnet34(weights=weights)\n","model.eval()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":426,"status":"ok","timestamp":1683411095829,"user":{"displayName":"Luca B","userId":"02373234078534734966"},"user_tz":-120},"id":"mJ0XDYGsREq8"},"outputs":[],"source":["def generate_embeddings():\n","    \"\"\"\n","    Transform, resize and normalize the images and then use a pretrained model to extract \n","    the embeddings.\n","    \"\"\"\n","    # Standardisieren hier die Daten auf z.B. selbe Dimensionen\n","    # Define a transform to pre-process the images; make pictures more similar \n","\n","    # 200x200 pixel transform transforms resize da manche zu gross/klein\n","\n","    train_transforms = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n","\n","    train_dataset = datasets.ImageFolder(root=dataset_dir, transform=train_transforms)\n","    # Hint: adjust batch_size and num_workers to your PC configuration, so that you don't \n","    # run out of memory\n","    train_loader = DataLoader(dataset=train_dataset,\n","                              batch_size=20,\n","                              shuffle=False,\n","                              pin_memory=True, num_workers=2) # num_workers = 2, batch_size = 20 \n","    print(train_loader)\n","    print(len(train_dataset))\n","\n","    # Use ResNet50 as pretrained model to get embeddings\n","    # remove the last layers of the model to access the embeddings the model generates. \n","    def copy_embeddings(module, input, output):\n","\n","      \"\"\"\n","      Copy embeddings from the penultimate layer.\n","      \"\"\"\n","\n","      output = output[:, :, 0, 0].detach().numpy().tolist()\n","\n","      outputs.append(output)\n","\n","    weights = ResNet34_Weights.DEFAULT\n","    model = resnet34(weights=weights)\n","    layer = model._modules.get('avgpool') # select desired layer\n","    _ = layer.register_forward_hook(copy_embeddings)\n","    embeddings = []\n","    embedding_size = 512 # Size of embeddings using ResNet18  \n","    model.eval() # activate the model\n","\n","    # Generate image's embeddings for all images in train_loader and saves \n","    # them in the list outputs\n","\n","    outputs = []\n","    counter = 0\n","    for img, label in train_loader:\n","        print(counter)\n","        counter+=1\n","        _ = model(img)\n","\n","    print(len(outputs)) # returns number of batches\n","    print(len(outputs[0])) # returns size of a batch\n","    print(len(outputs[7][9])) # returns embeddings_size\n","\n","    num_images = len(train_dataset)\n","    embeddings = np.zeros((num_images, embedding_size))\n","    print(embeddings.shape)\n","\n","    i = 0\n","    for batch in range(len(outputs)):\n","      print(i)\n","      for img in range(len(outputs[batch])):\n","        embeddings[i,:] = outputs[batch][img]\n","        i+=1\n","    \n","    np.save(embeddings_dir, embeddings)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KByT5HOwLdnO","executionInfo":{"status":"ok","timestamp":1683412311472,"user_tz":-120,"elapsed":1198318,"user":{"displayName":"Luca B","userId":"02373234078534734966"}},"outputId":"0fb6cf72-b0ad-4be6-fad3-e51cd2c86c86"},"outputs":[{"output_type":"stream","name":"stdout","text":["<torch.utils.data.dataloader.DataLoader object at 0x7fcb56a1be80>\n","10201\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n","220\n","221\n","222\n","223\n","224\n","225\n","226\n","227\n","228\n","229\n","230\n","231\n","232\n","233\n","234\n","235\n","236\n","237\n","238\n","239\n","240\n","241\n","242\n","243\n","244\n","245\n","246\n","247\n","248\n","249\n","250\n","251\n","252\n","253\n","254\n","255\n","256\n","257\n","258\n","259\n","260\n","261\n","262\n","263\n","264\n","265\n","266\n","267\n","268\n","269\n","270\n","271\n","272\n","273\n","274\n","275\n","276\n","277\n","278\n","279\n","280\n","281\n","282\n","283\n","284\n","285\n","286\n","287\n","288\n","289\n","290\n","291\n","292\n","293\n","294\n","295\n","296\n","297\n","298\n","299\n","300\n","301\n","302\n","303\n","304\n","305\n","306\n","307\n","308\n","309\n","310\n","311\n","312\n","313\n","314\n","315\n","316\n","317\n","318\n","319\n","320\n","321\n","322\n","323\n","324\n","325\n","326\n","327\n","328\n","329\n","330\n","331\n","332\n","333\n","334\n","335\n","336\n","337\n","338\n","339\n","340\n","341\n","342\n","343\n","344\n","345\n","346\n","347\n","348\n","349\n","350\n","351\n","352\n","353\n","354\n","355\n","356\n","357\n","358\n","359\n","360\n","361\n","362\n","363\n","364\n","365\n","366\n","367\n","368\n","369\n","370\n","371\n","372\n","373\n","374\n","375\n","376\n","377\n","378\n","379\n","380\n","381\n","382\n","383\n","384\n","385\n","386\n","387\n","388\n","389\n","390\n","391\n","392\n","393\n","394\n","395\n","396\n","397\n","398\n","399\n","400\n","401\n","402\n","403\n","404\n","405\n","406\n","407\n","408\n","409\n","410\n","411\n","412\n","413\n","414\n","415\n","416\n","417\n","418\n","419\n","420\n","421\n","422\n","423\n","424\n","425\n","426\n","427\n","428\n","429\n","430\n","431\n","432\n","433\n","434\n","435\n","436\n","437\n","438\n","439\n","440\n","441\n","442\n","443\n","444\n","445\n","446\n","447\n","448\n","449\n","450\n","451\n","452\n","453\n","454\n","455\n","456\n","457\n","458\n","459\n","460\n","461\n","462\n","463\n","464\n","465\n","466\n","467\n","468\n","469\n","470\n","471\n","472\n","473\n","474\n","475\n","476\n","477\n","478\n","479\n","480\n","481\n","482\n","483\n","484\n","485\n","486\n","487\n","488\n","489\n","490\n","491\n","492\n","493\n","494\n","495\n","496\n","497\n","498\n","499\n","500\n","501\n","502\n","503\n","504\n","505\n","506\n","507\n","508\n","509\n","510\n","511\n","20\n","512\n","(10201, 512)\n","0\n","20\n","40\n","60\n","80\n","100\n","120\n","140\n","160\n","180\n","200\n","220\n","240\n","260\n","280\n","300\n","320\n","340\n","360\n","380\n","400\n","420\n","440\n","460\n","480\n","500\n","520\n","540\n","560\n","580\n","600\n","620\n","640\n","660\n","680\n","700\n","720\n","740\n","760\n","780\n","800\n","820\n","840\n","860\n","880\n","900\n","920\n","940\n","960\n","980\n","1000\n","1020\n","1040\n","1060\n","1080\n","1100\n","1120\n","1140\n","1160\n","1180\n","1200\n","1220\n","1240\n","1260\n","1280\n","1300\n","1320\n","1340\n","1360\n","1380\n","1400\n","1420\n","1440\n","1460\n","1480\n","1500\n","1520\n","1540\n","1560\n","1580\n","1600\n","1620\n","1640\n","1660\n","1680\n","1700\n","1720\n","1740\n","1760\n","1780\n","1800\n","1820\n","1840\n","1860\n","1880\n","1900\n","1920\n","1940\n","1960\n","1980\n","2000\n","2020\n","2040\n","2060\n","2080\n","2100\n","2120\n","2140\n","2160\n","2180\n","2200\n","2220\n","2240\n","2260\n","2280\n","2300\n","2320\n","2340\n","2360\n","2380\n","2400\n","2420\n","2440\n","2460\n","2480\n","2500\n","2520\n","2540\n","2560\n","2580\n","2600\n","2620\n","2640\n","2660\n","2680\n","2700\n","2720\n","2740\n","2760\n","2780\n","2800\n","2820\n","2840\n","2860\n","2880\n","2900\n","2920\n","2940\n","2960\n","2980\n","3000\n","3020\n","3040\n","3060\n","3080\n","3100\n","3120\n","3140\n","3160\n","3180\n","3200\n","3220\n","3240\n","3260\n","3280\n","3300\n","3320\n","3340\n","3360\n","3380\n","3400\n","3420\n","3440\n","3460\n","3480\n","3500\n","3520\n","3540\n","3560\n","3580\n","3600\n","3620\n","3640\n","3660\n","3680\n","3700\n","3720\n","3740\n","3760\n","3780\n","3800\n","3820\n","3840\n","3860\n","3880\n","3900\n","3920\n","3940\n","3960\n","3980\n","4000\n","4020\n","4040\n","4060\n","4080\n","4100\n","4120\n","4140\n","4160\n","4180\n","4200\n","4220\n","4240\n","4260\n","4280\n","4300\n","4320\n","4340\n","4360\n","4380\n","4400\n","4420\n","4440\n","4460\n","4480\n","4500\n","4520\n","4540\n","4560\n","4580\n","4600\n","4620\n","4640\n","4660\n","4680\n","4700\n","4720\n","4740\n","4760\n","4780\n","4800\n","4820\n","4840\n","4860\n","4880\n","4900\n","4920\n","4940\n","4960\n","4980\n","5000\n","5020\n","5040\n","5060\n","5080\n","5100\n","5120\n","5140\n","5160\n","5180\n","5200\n","5220\n","5240\n","5260\n","5280\n","5300\n","5320\n","5340\n","5360\n","5380\n","5400\n","5420\n","5440\n","5460\n","5480\n","5500\n","5520\n","5540\n","5560\n","5580\n","5600\n","5620\n","5640\n","5660\n","5680\n","5700\n","5720\n","5740\n","5760\n","5780\n","5800\n","5820\n","5840\n","5860\n","5880\n","5900\n","5920\n","5940\n","5960\n","5980\n","6000\n","6020\n","6040\n","6060\n","6080\n","6100\n","6120\n","6140\n","6160\n","6180\n","6200\n","6220\n","6240\n","6260\n","6280\n","6300\n","6320\n","6340\n","6360\n","6380\n","6400\n","6420\n","6440\n","6460\n","6480\n","6500\n","6520\n","6540\n","6560\n","6580\n","6600\n","6620\n","6640\n","6660\n","6680\n","6700\n","6720\n","6740\n","6760\n","6780\n","6800\n","6820\n","6840\n","6860\n","6880\n","6900\n","6920\n","6940\n","6960\n","6980\n","7000\n","7020\n","7040\n","7060\n","7080\n","7100\n","7120\n","7140\n","7160\n","7180\n","7200\n","7220\n","7240\n","7260\n","7280\n","7300\n","7320\n","7340\n","7360\n","7380\n","7400\n","7420\n","7440\n","7460\n","7480\n","7500\n","7520\n","7540\n","7560\n","7580\n","7600\n","7620\n","7640\n","7660\n","7680\n","7700\n","7720\n","7740\n","7760\n","7780\n","7800\n","7820\n","7840\n","7860\n","7880\n","7900\n","7920\n","7940\n","7960\n","7980\n","8000\n","8020\n","8040\n","8060\n","8080\n","8100\n","8120\n","8140\n","8160\n","8180\n","8200\n","8220\n","8240\n","8260\n","8280\n","8300\n","8320\n","8340\n","8360\n","8380\n","8400\n","8420\n","8440\n","8460\n","8480\n","8500\n","8520\n","8540\n","8560\n","8580\n","8600\n","8620\n","8640\n","8660\n","8680\n","8700\n","8720\n","8740\n","8760\n","8780\n","8800\n","8820\n","8840\n","8860\n","8880\n","8900\n","8920\n","8940\n","8960\n","8980\n","9000\n","9020\n","9040\n","9060\n","9080\n","9100\n","9120\n","9140\n","9160\n","9180\n","9200\n","9220\n","9240\n","9260\n","9280\n","9300\n","9320\n","9340\n","9360\n","9380\n","9400\n","9420\n","9440\n","9460\n","9480\n","9500\n","9520\n","9540\n","9560\n","9580\n","9600\n","9620\n","9640\n","9660\n","9680\n","9700\n","9720\n","9740\n","9760\n","9780\n","9800\n","9820\n","9840\n","9860\n","9880\n","9900\n","9920\n","9940\n","9960\n","9980\n","10000\n","10020\n","10040\n","10060\n","10080\n","10100\n","10120\n","10140\n","10160\n","10180\n","10200\n"]}],"source":["generate_embeddings()"]},{"cell_type":"markdown","metadata":{"id":"NhJCAhDWuMgu"},"source":["# Find number of workers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":625,"status":"ok","timestamp":1683287728127,"user":{"displayName":"Luca B","userId":"02373234078534734966"},"user_tz":-120},"id":"f3cXhyBexAK2","outputId":"0e453538-4c94-4ac0-c2ca-e89ff1db6e69"},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n"]}],"source":["import multiprocessing as mp\n","print(mp.cpu_count())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":548,"status":"ok","timestamp":1683288021619,"user":{"displayName":"Luca B","userId":"02373234078534734966"},"user_tz":-120},"id":"fjS6WfImuL6e","outputId":"7f40ce96-416b-4967-e9ad-945e841c8d53"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finish with:0.04164481163024902 second, num_workers=2\n","Finish with:0.07740330696105957 second, num_workers=2\n"]}],"source":["from time import time\n","import multiprocessing as mp\n","\n","for num_workers in range(2, mp.cpu_count()+1, 2):  \n","    train_loader = DataLoader(sample_dir,shuffle=True,num_workers=num_workers,batch_size=50,pin_memory=True)\n","    start = time()\n","    for epoch in range(1, 3):\n","        for i, data in enumerate(train_loader, 0):\n","          pass\n","        end = time()\n","        print(\"Finish with:{} second, num_workers={}\".format(end - start, num_workers))"]},{"cell_type":"markdown","metadata":{"id":"thwBZNaB2CZj"},"source":["# Get Data"]},{"cell_type":"markdown","metadata":{"id":"YHUVqZOV77iX"},"source":["## Testing not relevant for main"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"elapsed":42228,"status":"error","timestamp":1683412643990,"user":{"displayName":"Luca B","userId":"02373234078534734966"},"user_tz":-120},"id":"7FhT1eA-7APz","outputId":"569ca3fc-1e14-436a-cf02-7a420a72abd3"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-24dae1945e22>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# generate training data from triplets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train_dataset = datasets.ImageFolder(dataset_dir,\n\u001b[0m\u001b[1;32m      8\u001b[0m                                      transform=None)\n\u001b[1;32m      9\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# is potentially overridden and thus could have a different logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# Note that scandir is global in this module due\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# to earlier import-*.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mscandir_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0monerror\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["    triplets = []\n","    with open(train_dir) as f:\n","        for line in f:\n","            triplets.append(line)\n","\n","    # generate training data from triplets\n","    train_dataset = datasets.ImageFolder(dataset_dir,\n","                                         transform=None)\n","    filenames = [s[0].split('/')[-1].replace('.jpg', '') for s in train_dataset.samples]\n","    print(filenames)\n","    embeddings = np.load(embeddings_dir)\n","    print(embeddings)\n","    embeddings = normalize(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32,"status":"aborted","timestamp":1683412643993,"user":{"displayName":"Luca B","userId":"02373234078534734966"},"user_tz":-120},"id":"BmLmdqct7aBM"},"outputs":[],"source":["print(embeddings.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":34,"status":"aborted","timestamp":1683412643997,"user":{"displayName":"Luca B","userId":"02373234078534734966"},"user_tz":-120},"id":"AiXtDM-H5mhc"},"outputs":[],"source":["print(len(filenames))"]},{"cell_type":"markdown","metadata":{"id":"a_zlbxInMnsC"},"source":["Für Train: Füge jedes Tripel 2 mal ein, als erste das Triple wo die ersten zwei ähnlich sind und füge eine 1 für die korrekte Ähnlichkeit im Geschmack ein und eine 0 wenn sie nicht ähnlich schmecken in y für den zweiten Fall; X enthält die Embeddings (2 mal; in der ersten Zeile die Ähnlichkeit; in der zweiten das andere\n","\n","Falls nicht train: gib nur einmal Triple weiter (das mit 1) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WthxWRdg4_Q9","executionInfo":{"status":"aborted","timestamp":1683412643998,"user_tz":-120,"elapsed":34,"user":{"displayName":"Luca B","userId":"02373234078534734966"}}},"outputs":[],"source":["    file_to_embedding = {}\n","    for i in range(len(filenames)):\n","        file_to_embedding[filenames[i]] = embeddings[i]\n","    X = []\n","    y = []\n","    counter = 0\n","    # use the individual embeddings to generate the features and labels for triplets\n","    for t in triplets:\n","        counter += 1\n","        if counter > 1:\n","          break\n","        emb = [file_to_embedding[a] for a in t.split()] # embedding triplets of pictures\n","        X.append(np.hstack([emb[0], emb[1], emb[2]])) # [emb[0], emb[1], emb[2]] merge the embeddings vertically\n","        # hstack merges them as a new column\n","        y.append(1)\n","        # Generating negative samples (data augmentation)\n","        X.append(np.hstack([emb[0], emb[2], emb[1]]))\n","        y.append(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":36,"status":"aborted","timestamp":1683412644001,"user":{"displayName":"Luca B","userId":"02373234078534734966"},"user_tz":-120},"id":"lpXbhBZ-8r8s"},"outputs":[],"source":["print(X)\n","print(y)"]},{"cell_type":"markdown","metadata":{"id":"3hnYmt5s7_FL"},"source":["## Definition"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8wELbfrXVwHJ","executionInfo":{"status":"ok","timestamp":1683412649848,"user_tz":-120,"elapsed":267,"user":{"displayName":"Luca B","userId":"02373234078534734966"}}},"outputs":[],"source":["def get_data(file, train=True):\n","    \"\"\"\n","    Load the triplets from the file and generate the features and labels.\n","\n","    input: file: string, the path to the file containing the triplets\n","          train: boolean, whether the data is for training or testing\n","\n","    output: X: numpy array, the features, each column contains all embeddings of the pictures\n","            y: numpy array, the labels, 1 if the first picture tastes similar to the second picture\n","    \"\"\"\n","    triplets = []\n","    with open(file) as f:\n","        for line in f:\n","            triplets.append(line)\n","\n","    # generate training data from triplets\n","    train_dataset = datasets.ImageFolder(dataset_dir,\n","                                         transform=None)\n","    filenames = [s[0].split('/')[-1].replace('.jpg', '') for s in train_dataset.samples]\n","    embeddings = np.load(embeddings_dir)\n","    embeddings = normalize(embeddings) # Normalize the embeddings across the dataset\n","  \n","\n","    file_to_embedding = {}\n","    for i in range(len(filenames)):\n","        file_to_embedding[filenames[i]] = embeddings[i]\n","    X = []\n","    y = []\n","    # use the individual embeddings to generate the features and labels for triplets\n","    for t in triplets:\n","        emb = [file_to_embedding[a] for a in t.split()]\n","        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n","        y.append(1)\n","        # Generating negative samples (data augmentation)\n","        if train:\n","            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n","            y.append(0)   \n","    X = np.vstack(X) # vertical stack\n","    y = np.hstack(y) # horizonal stack\n","\n","    return X, y"]},{"cell_type":"markdown","metadata":{"id":"qncf-gAh2NGV"},"source":["# Create Loader From Np"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CM44k2eW2Oyn","executionInfo":{"status":"ok","timestamp":1683412652364,"user_tz":-120,"elapsed":323,"user":{"displayName":"Luca B","userId":"02373234078534734966"}}},"outputs":[],"source":["# Hint: adjust batch_size and num_workers to your PC configuration, so that you don't run out of memory\n","def create_loader_from_np(X, y = None, train = True, batch_size=20, shuffle=True, num_workers = 2):\n","    \"\"\"\n","    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n","\n","    input: X: numpy array, the features\n","           y: numpy array, the labels\n","    \n","    output: loader: torch.data.util.DataLoader, the object containing the data\n","    \"\"\"\n","    if train:\n","        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), \n","                                torch.from_numpy(y).type(torch.long))\n","    else:\n","        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n","    loader = DataLoader(dataset=dataset,\n","                        batch_size=batch_size,\n","                        shuffle=shuffle,\n","                        pin_memory=True, num_workers=num_workers)\n","    return loader"]},{"cell_type":"markdown","metadata":{"id":"b_oKbUMZ2qMu"},"source":["# Create Neural Network"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"a07QJ6Iv2sgz","executionInfo":{"status":"ok","timestamp":1683414358663,"user_tz":-120,"elapsed":313,"user":{"displayName":"Luca B","userId":"02373234078534734966"}}},"outputs":[],"source":["# TODO: define a model. Here, the basic structure is defined, but you need to fill in the details\n","class Net(nn.Module):\n","    \"\"\"\n","    The model class, which defines our classifier.\n","    \"\"\"\n","    \n","    def __init__(self):\n","        \n","        # The constructor of the model.\n","        \n","        super().__init__()\n","        self.fc1 = nn.Linear(3*512,512)\n","        self.fc2 = nn.Linear(512,256)\n","        self.fc3 = nn.Linear(256,123)\n","        self.out = nn.Linear(123,1)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        The forward pass of the model.\n","\n","        input: x: torch.Tensor, the input to the model\n","\n","        output: x: torch.Tensor, the output of the model\n","        \"\"\"\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = self.out(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"Z8mMLJ8i23AE"},"source":["# Train Model"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"TXYJOH3T25Et","executionInfo":{"status":"ok","timestamp":1683412654864,"user_tz":-120,"elapsed":8,"user":{"displayName":"Luca B","userId":"02373234078534734966"}}},"outputs":[],"source":["def train_model(train_loader):\n","    \"\"\"\n","    The training procedure of the model; it accepts the training data, defines the model \n","    and then trains it.\n","\n","    input: train_loader: torch.data.util.DataLoader, the object containing the training data\n","    \n","    output: model: torch.nn.Module, the trained model\n","    \"\"\"\n","    #os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","    model = Net()\n","    model.train()\n","    model.to(device)\n","    m = nn.Sigmoid()\n","    loss_function = nn.BCELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","    \n","    losses = []\n","    n_epochs = 5 # 5 the best\n","    \n","\n","\n","    # TODO: define a loss function, optimizer and proceed with training. Hint: use the part \n","    # of the training data as a validation split. After each epoch, compute the loss on the \n","    # validation split and print it out. This enables you to see how your model is performing \n","    # on the validation data before submitting the results on the server. After choosing the \n","    # best model, train it on the whole training data.\n","    for epoch in range(n_epochs):    \n","        for batch_idx, (X, y) in enumerate(train_loader):\n","          optimizer.zero_grad()         \n","          output = model.forward(X)\n","          #print(output)\n","          output = torch.squeeze(output)\n","          #print(m(output))\n","          #print(y)\n","          loss = loss_function(m(output), y.float())\n","          losses.append(loss) \n","          loss.backward()\n","          optimizer.step()\n","          if batch_idx % 100 == 0:\n","            print('Epoch {}, Batch idx {}, loss {}'.format(\n","                   epoch, batch_idx, loss.item()))\n","                  \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"jMLNZysP3BCp"},"source":["# Test Model"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Btj3kkhc3CJ3","executionInfo":{"status":"ok","timestamp":1683412660443,"user_tz":-120,"elapsed":372,"user":{"displayName":"Luca B","userId":"02373234078534734966"}}},"outputs":[],"source":["def test_model(model, loader):\n","    \"\"\"\n","    The testing procedure of the model; it accepts the testing data and the trained model and \n","    then tests the model on it.\n","\n","    input: model: torch.nn.Module, the trained model\n","           loader: torch.data.util.DataLoader, the object containing the testing data\n","        \n","    output: None, the function saves the predictions to a results.txt file\n","    \"\"\"\n","    model.eval()\n","    predictions = []\n","    # Iterate over the test data\n","    with torch.no_grad(): # We don't need to compute gradients for testing\n","        for [x_batch] in loader:\n","            x_batch= x_batch.to(device)\n","            predicted = model(x_batch)\n","            predicted = predicted.cpu().numpy()\n","            # Rounding the predictions to 0 or 1\n","            predicted[predicted >= 0.5] = 1\n","            predicted[predicted < 0.5] = 0\n","            predictions.append(predicted)\n","        predictions = np.vstack(predictions)\n","    np.savetxt(results_dir, predictions, fmt='%i')"]},{"cell_type":"markdown","metadata":{"id":"xZMYr7xe3F6E"},"source":["# Main Function"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"kwHHoFxK3HSp","executionInfo":{"status":"ok","timestamp":1683412662814,"user_tz":-120,"elapsed":335,"user":{"displayName":"Luca B","userId":"02373234078534734966"}}},"outputs":[],"source":["# Main function. You don't have to change this\n","if __name__ == '__main__':\n","    TRAIN_TRIPLETS = train_dir\n","    TEST_TRIPLETS = test_dir\n","\n","    # generate embedding for each image in the dataset\n","    if(os.path.exists(embeddings_dir) == False):\n","        generate_embeddings()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ZTsDPZDNMxL9","executionInfo":{"status":"ok","timestamp":1683412671027,"user_tz":-120,"elapsed":6728,"user":{"displayName":"Luca B","userId":"02373234078534734966"}}},"outputs":[],"source":["    # load the training and testing data \n","    X, y = get_data(TRAIN_TRIPLETS)\n","    X_test, _ = get_data(TEST_TRIPLETS, train=False)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"kGln32aLM1MF","executionInfo":{"status":"ok","timestamp":1683412671798,"user_tz":-120,"elapsed":401,"user":{"displayName":"Luca B","userId":"02373234078534734966"}}},"outputs":[],"source":["    # Create data loaders for the training and testing data\n","    train_loader = create_loader_from_np(X, y, train = True, batch_size=20)\n","    test_loader = create_loader_from_np(X_test, train = False, batch_size=20, shuffle=False)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":723017,"status":"ok","timestamp":1683415086737,"user":{"displayName":"Luca B","userId":"02373234078534734966"},"user_tz":-120},"id":"GZ6KEfwQM2ac","outputId":"b1816199-2a2d-4cf0-90ef-ed2f584c7d98"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Batch idx 0, loss 0.6900358200073242\n","Epoch 0, Batch idx 100, loss 0.6986687779426575\n","Epoch 0, Batch idx 200, loss 0.695382297039032\n","Epoch 0, Batch idx 300, loss 0.6959062814712524\n","Epoch 0, Batch idx 400, loss 0.693827211856842\n","Epoch 0, Batch idx 500, loss 0.689926028251648\n","Epoch 0, Batch idx 600, loss 0.6855530738830566\n","Epoch 0, Batch idx 700, loss 0.7022871971130371\n","Epoch 0, Batch idx 800, loss 0.6834679841995239\n","Epoch 0, Batch idx 900, loss 0.6923227310180664\n","Epoch 0, Batch idx 1000, loss 0.6856356859207153\n","Epoch 0, Batch idx 1100, loss 0.6926175951957703\n","Epoch 0, Batch idx 1200, loss 0.6874204874038696\n","Epoch 0, Batch idx 1300, loss 0.7272890210151672\n","Epoch 0, Batch idx 1400, loss 0.723486065864563\n","Epoch 0, Batch idx 1500, loss 0.7706869840621948\n","Epoch 0, Batch idx 1600, loss 0.6575467586517334\n","Epoch 0, Batch idx 1700, loss 0.5840273499488831\n","Epoch 0, Batch idx 1800, loss 0.6673463582992554\n","Epoch 0, Batch idx 1900, loss 0.5777016878128052\n","Epoch 0, Batch idx 2000, loss 0.5786219835281372\n","Epoch 0, Batch idx 2100, loss 0.7644141912460327\n","Epoch 0, Batch idx 2200, loss 0.4905366897583008\n","Epoch 0, Batch idx 2300, loss 0.6193574666976929\n","Epoch 0, Batch idx 2400, loss 0.8957006335258484\n","Epoch 0, Batch idx 2500, loss 0.6863319277763367\n","Epoch 0, Batch idx 2600, loss 0.660171627998352\n","Epoch 0, Batch idx 2700, loss 0.666228175163269\n","Epoch 0, Batch idx 2800, loss 0.5995825529098511\n","Epoch 0, Batch idx 2900, loss 0.6575546860694885\n","Epoch 0, Batch idx 3000, loss 0.7273529767990112\n","Epoch 0, Batch idx 3100, loss 0.7392325401306152\n","Epoch 0, Batch idx 3200, loss 0.6503844261169434\n","Epoch 0, Batch idx 3300, loss 0.6140555143356323\n","Epoch 0, Batch idx 3400, loss 0.5445040464401245\n","Epoch 0, Batch idx 3500, loss 0.6047585606575012\n","Epoch 0, Batch idx 3600, loss 0.48498114943504333\n","Epoch 0, Batch idx 3700, loss 0.5800508260726929\n","Epoch 0, Batch idx 3800, loss 0.5852136611938477\n","Epoch 0, Batch idx 3900, loss 0.6140220165252686\n","Epoch 0, Batch idx 4000, loss 0.7081491947174072\n","Epoch 0, Batch idx 4100, loss 0.6867186427116394\n","Epoch 0, Batch idx 4200, loss 0.6273666620254517\n","Epoch 0, Batch idx 4300, loss 0.6844232082366943\n","Epoch 0, Batch idx 4400, loss 0.5743231773376465\n","Epoch 0, Batch idx 4500, loss 0.7870498895645142\n","Epoch 0, Batch idx 4600, loss 0.6310782432556152\n","Epoch 0, Batch idx 4700, loss 0.6263110637664795\n","Epoch 0, Batch idx 4800, loss 0.4888543486595154\n","Epoch 0, Batch idx 4900, loss 0.5962293744087219\n","Epoch 0, Batch idx 5000, loss 0.6921557188034058\n","Epoch 0, Batch idx 5100, loss 0.5199055075645447\n","Epoch 0, Batch idx 5200, loss 0.6611801385879517\n","Epoch 0, Batch idx 5300, loss 0.5610061883926392\n","Epoch 0, Batch idx 5400, loss 0.6649729013442993\n","Epoch 0, Batch idx 5500, loss 0.7234293818473816\n","Epoch 0, Batch idx 5600, loss 0.6670428514480591\n","Epoch 0, Batch idx 5700, loss 0.6114554405212402\n","Epoch 0, Batch idx 5800, loss 0.5705341100692749\n","Epoch 0, Batch idx 5900, loss 0.6004698872566223\n","Epoch 1, Batch idx 0, loss 0.4503191411495209\n","Epoch 1, Batch idx 100, loss 0.4029790461063385\n","Epoch 1, Batch idx 200, loss 0.5110185742378235\n","Epoch 1, Batch idx 300, loss 0.5793794393539429\n","Epoch 1, Batch idx 400, loss 0.5011870265007019\n","Epoch 1, Batch idx 500, loss 0.6388570070266724\n","Epoch 1, Batch idx 600, loss 0.5165011286735535\n","Epoch 1, Batch idx 700, loss 0.5121771693229675\n","Epoch 1, Batch idx 800, loss 0.5007852911949158\n","Epoch 1, Batch idx 900, loss 0.5052653551101685\n","Epoch 1, Batch idx 1000, loss 0.6014906764030457\n","Epoch 1, Batch idx 1100, loss 0.5177061557769775\n","Epoch 1, Batch idx 1200, loss 0.6952027678489685\n","Epoch 1, Batch idx 1300, loss 0.6060771942138672\n","Epoch 1, Batch idx 1400, loss 0.6131361722946167\n","Epoch 1, Batch idx 1500, loss 0.4809940457344055\n","Epoch 1, Batch idx 1600, loss 0.517148494720459\n","Epoch 1, Batch idx 1700, loss 0.5423228740692139\n","Epoch 1, Batch idx 1800, loss 0.44336146116256714\n","Epoch 1, Batch idx 1900, loss 0.3348512053489685\n","Epoch 1, Batch idx 2000, loss 0.7191721200942993\n","Epoch 1, Batch idx 2100, loss 0.4361971318721771\n","Epoch 1, Batch idx 2200, loss 0.5090019106864929\n","Epoch 1, Batch idx 2300, loss 0.4509252905845642\n","Epoch 1, Batch idx 2400, loss 0.6358872652053833\n","Epoch 1, Batch idx 2500, loss 0.45853519439697266\n","Epoch 1, Batch idx 2600, loss 0.40068191289901733\n","Epoch 1, Batch idx 2700, loss 0.563309371471405\n","Epoch 1, Batch idx 2800, loss 0.5354595184326172\n","Epoch 1, Batch idx 2900, loss 0.431649774312973\n","Epoch 1, Batch idx 3000, loss 0.562816858291626\n","Epoch 1, Batch idx 3100, loss 0.7097436189651489\n","Epoch 1, Batch idx 3200, loss 0.5652596950531006\n","Epoch 1, Batch idx 3300, loss 0.6192158460617065\n","Epoch 1, Batch idx 3400, loss 0.4061872065067291\n","Epoch 1, Batch idx 3500, loss 0.3350713551044464\n","Epoch 1, Batch idx 3600, loss 0.40676841139793396\n","Epoch 1, Batch idx 3700, loss 0.3994787037372589\n","Epoch 1, Batch idx 3800, loss 0.613556981086731\n","Epoch 1, Batch idx 3900, loss 0.5279536843299866\n","Epoch 1, Batch idx 4000, loss 0.6455346345901489\n","Epoch 1, Batch idx 4100, loss 0.5628656148910522\n","Epoch 1, Batch idx 4200, loss 0.6443920135498047\n","Epoch 1, Batch idx 4300, loss 0.5254038572311401\n","Epoch 1, Batch idx 4400, loss 0.5669183135032654\n","Epoch 1, Batch idx 4500, loss 0.5897766351699829\n","Epoch 1, Batch idx 4600, loss 0.5777854919433594\n","Epoch 1, Batch idx 4700, loss 0.5276626944541931\n","Epoch 1, Batch idx 4800, loss 0.35420066118240356\n","Epoch 1, Batch idx 4900, loss 0.5078898668289185\n","Epoch 1, Batch idx 5000, loss 0.6396456956863403\n","Epoch 1, Batch idx 5100, loss 0.48787274956703186\n","Epoch 1, Batch idx 5200, loss 0.6110727190971375\n","Epoch 1, Batch idx 5300, loss 0.561538577079773\n","Epoch 1, Batch idx 5400, loss 0.6125842332839966\n","Epoch 1, Batch idx 5500, loss 0.5313907861709595\n","Epoch 1, Batch idx 5600, loss 0.683308482170105\n","Epoch 1, Batch idx 5700, loss 0.6365313529968262\n","Epoch 1, Batch idx 5800, loss 0.5980130434036255\n","Epoch 1, Batch idx 5900, loss 0.6385168433189392\n","Epoch 2, Batch idx 0, loss 0.7726992964744568\n","Epoch 2, Batch idx 100, loss 0.5343260169029236\n","Epoch 2, Batch idx 200, loss 0.5501906275749207\n","Epoch 2, Batch idx 300, loss 0.48163801431655884\n","Epoch 2, Batch idx 400, loss 0.6392275094985962\n","Epoch 2, Batch idx 500, loss 0.6932857036590576\n","Epoch 2, Batch idx 600, loss 0.6012091636657715\n","Epoch 2, Batch idx 700, loss 0.486259788274765\n","Epoch 2, Batch idx 800, loss 0.48013824224472046\n","Epoch 2, Batch idx 900, loss 0.47369542717933655\n","Epoch 2, Batch idx 1000, loss 0.652931809425354\n","Epoch 2, Batch idx 1100, loss 0.5651423335075378\n","Epoch 2, Batch idx 1200, loss 0.556658148765564\n","Epoch 2, Batch idx 1300, loss 0.6259158253669739\n","Epoch 2, Batch idx 1400, loss 0.5166712999343872\n","Epoch 2, Batch idx 1500, loss 0.6055208444595337\n","Epoch 2, Batch idx 1600, loss 0.5131268501281738\n","Epoch 2, Batch idx 1700, loss 0.562167227268219\n","Epoch 2, Batch idx 1800, loss 0.5274025797843933\n","Epoch 2, Batch idx 1900, loss 0.2929095923900604\n","Epoch 2, Batch idx 2000, loss 0.47715264558792114\n","Epoch 2, Batch idx 2100, loss 0.3919803202152252\n","Epoch 2, Batch idx 2200, loss 0.5655080080032349\n","Epoch 2, Batch idx 2300, loss 0.48578396439552307\n","Epoch 2, Batch idx 2400, loss 0.7945823073387146\n","Epoch 2, Batch idx 2500, loss 0.6063601970672607\n","Epoch 2, Batch idx 2600, loss 0.5465200543403625\n","Epoch 2, Batch idx 2700, loss 0.6921892166137695\n","Epoch 2, Batch idx 2800, loss 0.384233295917511\n","Epoch 2, Batch idx 2900, loss 0.6397632360458374\n","Epoch 2, Batch idx 3000, loss 0.5438793897628784\n","Epoch 2, Batch idx 3100, loss 0.46916645765304565\n","Epoch 2, Batch idx 3200, loss 0.4874514937400818\n","Epoch 2, Batch idx 3300, loss 0.352347195148468\n","Epoch 2, Batch idx 3400, loss 0.5822176337242126\n","Epoch 2, Batch idx 3500, loss 0.7172614336013794\n","Epoch 2, Batch idx 3600, loss 0.5150033235549927\n","Epoch 2, Batch idx 3700, loss 0.5325425863265991\n","Epoch 2, Batch idx 3800, loss 0.6669472455978394\n","Epoch 2, Batch idx 3900, loss 0.4788723886013031\n","Epoch 2, Batch idx 4000, loss 0.5920584201812744\n","Epoch 2, Batch idx 4100, loss 0.3952423334121704\n","Epoch 2, Batch idx 4200, loss 0.5745401382446289\n","Epoch 2, Batch idx 4300, loss 0.5595408082008362\n","Epoch 2, Batch idx 4400, loss 0.5763615965843201\n","Epoch 2, Batch idx 4500, loss 0.5133094787597656\n","Epoch 2, Batch idx 4600, loss 0.6203621625900269\n","Epoch 2, Batch idx 4700, loss 0.4522557258605957\n","Epoch 2, Batch idx 4800, loss 0.46342819929122925\n","Epoch 2, Batch idx 4900, loss 0.8642808198928833\n","Epoch 2, Batch idx 5000, loss 0.6403932571411133\n","Epoch 2, Batch idx 5100, loss 0.6074928045272827\n","Epoch 2, Batch idx 5200, loss 0.6012080311775208\n","Epoch 2, Batch idx 5300, loss 0.5408282279968262\n","Epoch 2, Batch idx 5400, loss 0.7305782437324524\n","Epoch 2, Batch idx 5500, loss 0.5974699258804321\n","Epoch 2, Batch idx 5600, loss 0.5189852714538574\n","Epoch 2, Batch idx 5700, loss 0.5078282356262207\n","Epoch 2, Batch idx 5800, loss 0.6115317940711975\n","Epoch 2, Batch idx 5900, loss 0.48281335830688477\n","Epoch 3, Batch idx 0, loss 0.5711616277694702\n","Epoch 3, Batch idx 100, loss 0.5060638785362244\n","Epoch 3, Batch idx 200, loss 0.40438660979270935\n","Epoch 3, Batch idx 300, loss 0.5003922581672668\n","Epoch 3, Batch idx 400, loss 0.6729450225830078\n","Epoch 3, Batch idx 500, loss 0.5233838558197021\n","Epoch 3, Batch idx 600, loss 0.48054593801498413\n","Epoch 3, Batch idx 700, loss 0.5574338436126709\n","Epoch 3, Batch idx 800, loss 0.470526784658432\n","Epoch 3, Batch idx 900, loss 0.6098440885543823\n","Epoch 3, Batch idx 1000, loss 0.528209924697876\n","Epoch 3, Batch idx 1100, loss 0.6008120179176331\n","Epoch 3, Batch idx 1200, loss 0.5789374113082886\n","Epoch 3, Batch idx 1300, loss 0.36036938428878784\n","Epoch 3, Batch idx 1400, loss 0.6420860886573792\n","Epoch 3, Batch idx 1500, loss 0.7078352570533752\n","Epoch 3, Batch idx 1600, loss 0.5118095874786377\n","Epoch 3, Batch idx 1700, loss 0.5278534293174744\n","Epoch 3, Batch idx 1800, loss 0.5878067016601562\n","Epoch 3, Batch idx 1900, loss 0.5160542726516724\n","Epoch 3, Batch idx 2000, loss 0.5205975770950317\n","Epoch 3, Batch idx 2100, loss 0.5654040575027466\n","Epoch 3, Batch idx 2200, loss 0.41129082441329956\n","Epoch 3, Batch idx 2300, loss 0.5784033536911011\n","Epoch 3, Batch idx 2400, loss 0.7973640561103821\n","Epoch 3, Batch idx 2500, loss 0.7008102536201477\n","Epoch 3, Batch idx 2600, loss 0.6219131946563721\n","Epoch 3, Batch idx 2700, loss 0.7244112491607666\n","Epoch 3, Batch idx 2800, loss 0.795683741569519\n","Epoch 3, Batch idx 2900, loss 0.47985729575157166\n","Epoch 3, Batch idx 3000, loss 0.49932020902633667\n","Epoch 3, Batch idx 3100, loss 0.6236902475357056\n","Epoch 3, Batch idx 3200, loss 0.6557032465934753\n","Epoch 3, Batch idx 3300, loss 0.5161901712417603\n","Epoch 3, Batch idx 3400, loss 0.7000552415847778\n","Epoch 3, Batch idx 3500, loss 0.43881577253341675\n","Epoch 3, Batch idx 3600, loss 0.5613362193107605\n","Epoch 3, Batch idx 3700, loss 0.7390033006668091\n","Epoch 3, Batch idx 3800, loss 0.5268759727478027\n","Epoch 3, Batch idx 3900, loss 0.5132834315299988\n","Epoch 3, Batch idx 4000, loss 0.4992227554321289\n","Epoch 3, Batch idx 4100, loss 0.6018438935279846\n","Epoch 3, Batch idx 4200, loss 0.30763405561447144\n","Epoch 3, Batch idx 4300, loss 0.45095816254615784\n","Epoch 3, Batch idx 4400, loss 0.41469520330429077\n","Epoch 3, Batch idx 4500, loss 0.4192703366279602\n","Epoch 3, Batch idx 4600, loss 0.48790016770362854\n","Epoch 3, Batch idx 4700, loss 0.5248256325721741\n","Epoch 3, Batch idx 4800, loss 0.4761810302734375\n","Epoch 3, Batch idx 4900, loss 0.5455141067504883\n","Epoch 3, Batch idx 5000, loss 0.4365878105163574\n","Epoch 3, Batch idx 5100, loss 0.5742121934890747\n","Epoch 3, Batch idx 5200, loss 0.8739962577819824\n","Epoch 3, Batch idx 5300, loss 0.588623046875\n","Epoch 3, Batch idx 5400, loss 0.514306902885437\n","Epoch 3, Batch idx 5500, loss 0.564038097858429\n","Epoch 3, Batch idx 5600, loss 0.47526925802230835\n","Epoch 3, Batch idx 5700, loss 0.4801488518714905\n","Epoch 3, Batch idx 5800, loss 0.40930166840553284\n","Epoch 3, Batch idx 5900, loss 0.5726283192634583\n","Epoch 4, Batch idx 0, loss 0.43665003776550293\n","Epoch 4, Batch idx 100, loss 0.6864385604858398\n","Epoch 4, Batch idx 200, loss 0.5454022288322449\n","Epoch 4, Batch idx 300, loss 0.5015066266059875\n","Epoch 4, Batch idx 400, loss 0.5647130012512207\n","Epoch 4, Batch idx 500, loss 0.48601460456848145\n","Epoch 4, Batch idx 600, loss 0.42558223009109497\n","Epoch 4, Batch idx 700, loss 0.544654369354248\n","Epoch 4, Batch idx 800, loss 0.4797515869140625\n","Epoch 4, Batch idx 900, loss 0.5477983355522156\n","Epoch 4, Batch idx 1000, loss 0.6073530912399292\n","Epoch 4, Batch idx 1100, loss 0.5850162506103516\n","Epoch 4, Batch idx 1200, loss 0.7966230511665344\n","Epoch 4, Batch idx 1300, loss 0.5774650573730469\n","Epoch 4, Batch idx 1400, loss 0.6917296648025513\n","Epoch 4, Batch idx 1500, loss 0.4745451509952545\n","Epoch 4, Batch idx 1600, loss 0.6612149477005005\n","Epoch 4, Batch idx 1700, loss 0.3992670476436615\n","Epoch 4, Batch idx 1800, loss 0.3946987986564636\n","Epoch 4, Batch idx 1900, loss 0.44798654317855835\n","Epoch 4, Batch idx 2000, loss 0.6172730326652527\n","Epoch 4, Batch idx 2100, loss 0.448957622051239\n","Epoch 4, Batch idx 2200, loss 0.6034181714057922\n","Epoch 4, Batch idx 2300, loss 0.3788139820098877\n","Epoch 4, Batch idx 2400, loss 0.6733242869377136\n","Epoch 4, Batch idx 2500, loss 0.5720794200897217\n","Epoch 4, Batch idx 2600, loss 0.7810760140419006\n","Epoch 4, Batch idx 2700, loss 0.5911502242088318\n","Epoch 4, Batch idx 2800, loss 0.4508749842643738\n","Epoch 4, Batch idx 2900, loss 0.535639762878418\n","Epoch 4, Batch idx 3000, loss 0.5443008542060852\n","Epoch 4, Batch idx 3100, loss 0.6621143817901611\n","Epoch 4, Batch idx 3200, loss 0.6442604064941406\n","Epoch 4, Batch idx 3300, loss 0.5496197938919067\n","Epoch 4, Batch idx 3400, loss 0.5350911617279053\n","Epoch 4, Batch idx 3500, loss 0.35019952058792114\n","Epoch 4, Batch idx 3600, loss 0.5111571550369263\n","Epoch 4, Batch idx 3700, loss 0.5404519438743591\n","Epoch 4, Batch idx 3800, loss 0.6026962995529175\n","Epoch 4, Batch idx 3900, loss 0.7297955751419067\n","Epoch 4, Batch idx 4000, loss 0.6309154033660889\n","Epoch 4, Batch idx 4100, loss 0.685120701789856\n","Epoch 4, Batch idx 4200, loss 0.4589982032775879\n","Epoch 4, Batch idx 4300, loss 0.5008788108825684\n","Epoch 4, Batch idx 4400, loss 0.5029016733169556\n","Epoch 4, Batch idx 4500, loss 0.45089584589004517\n","Epoch 4, Batch idx 4600, loss 0.5399426221847534\n","Epoch 4, Batch idx 4700, loss 0.5488590002059937\n","Epoch 4, Batch idx 4800, loss 0.33012956380844116\n","Epoch 4, Batch idx 4900, loss 0.448003351688385\n","Epoch 4, Batch idx 5000, loss 0.3744373917579651\n","Epoch 4, Batch idx 5100, loss 0.4890400767326355\n","Epoch 4, Batch idx 5200, loss 0.5845019817352295\n","Epoch 4, Batch idx 5300, loss 0.721672534942627\n","Epoch 4, Batch idx 5400, loss 0.5136165618896484\n","Epoch 4, Batch idx 5500, loss 0.5060086846351624\n","Epoch 4, Batch idx 5600, loss 0.41451072692871094\n","Epoch 4, Batch idx 5700, loss 0.5891508460044861\n","Epoch 4, Batch idx 5800, loss 0.4676523208618164\n","Epoch 4, Batch idx 5900, loss 0.2845352292060852\n"]}],"source":["    # define a model and train it\n","    model = train_model(train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRpOpZSuM4Xi"},"outputs":[],"source":["    # test the model on the test data\n","    test_model(model, test_loader)\n","    print(\"Results saved to results.txt\")"]},{"cell_type":"markdown","metadata":{"id":"fCxizgISyreQ"},"source":["## Test - nor relevant for main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xa8YdVNryYFo"},"outputs":[],"source":["    TRAIN_TRIPLETS = train_dir\n","    TEST_TRIPLETS = test_dir\n","\n","    # generate embedding for each image in the dataset\n","    if(os.path.exists(embeddings_dir) == False):\n","        print(\"generate embeddings\")\n","        generate_embeddings()\n","        # load the training and testing data "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPHr8oSBy6lq"},"outputs":[],"source":["X, y = get_data(TRAIN_TRIPLETS)\n","X_test, _ = get_data(TEST_TRIPLETS, train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1683306922572,"user":{"displayName":"Luca B","userId":"02373234078534734966"},"user_tz":-120},"id":"_7NgRaLh6Mwg","outputId":"8b7c4c44-d29d-4990-dc08-44543d31d08f"},"outputs":[{"name":"stdout","output_type":"stream","text":["1536\n","119030\n","119030\n"]}],"source":["print(len(X[0]))\n","print(len(X))\n","print(len(y))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCrddez-AnRh"},"outputs":[],"source":["train_loader = create_loader_from_np(X, y, train = True, batch_size=20)\n","test_loader = create_loader_from_np(X_test, train = False, batch_size=20, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252,"status":"ok","timestamp":1683305091069,"user":{"displayName":"Luca B","userId":"02373234078534734966"},"user_tz":-120},"id":"zxI0pyhzwoeU","outputId":"e81feb58-db96-41fd-b21d-b15e0bcf09e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["<enumerate object at 0x7fb0f3575540>\n"]}],"source":[]}],"metadata":{"colab":{"collapsed_sections":["NhJCAhDWuMgu","thwBZNaB2CZj","YHUVqZOV77iX","3hnYmt5s7_FL","qncf-gAh2NGV","jMLNZysP3BCp"],"provenance":[],"mount_file_id":"1ue3_G6Mn9vwoWxVumhyFr5-VR2nFBRR9","authorship_tag":"ABX9TyNWWTe1fiodXsg+Oqjb4EsF"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}